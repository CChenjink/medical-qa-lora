# QLoRA 微调配置 - 40k 数据规模

# 模型配置
model_name_or_path: "Qwen/Qwen2.5-3B-Instruct"
model_type: "qwen2"

# 量化配置
quantization_config:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"

# LoRA 配置
lora_config:
  r: 8
  lora_alpha: 32
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# 训练配置
training_args:
  output_dir: "./outputs/qlora_40k"
  num_train_epochs: 3
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 1.0e-4
  warmup_ratio: 0.1
  logging_steps: 200
  save_steps: 2400
  eval_steps: 2400
  save_total_limit: 3
  fp16: true
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "loss"
  greater_is_better: false

# 数据配置
data_config:
  train_file: "./data/processed/train_40k.json"
  validation_file: "./data/processed/dev.json"
  test_file: "./data/processed/test.json"
  max_source_length: 512
  max_target_length: 512
  max_samples: 40000

# 其他配置
seed: 42
