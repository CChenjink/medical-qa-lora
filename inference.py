"""
äº¤äº’å¼æ¨ç†è„šæœ¬
ç”¨äºæµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
"""

import argparse
import torch

# ä» src æ¨¡å—å¯¼å…¥åŠŸèƒ½
from src.model import load_trained_model


def chat(model, tokenizer, instruction="å›ç­”åŒ»ç–—å¥åº·é—®é¢˜"):
    """äº¤äº’å¼å¯¹è¯"""
    print("=" * 50)
    print("ğŸ¥ åŒ»ç–—é—®ç­”åŠ©æ‰‹")
    print("=" * 50)
    print("è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'clear' æ¸…å±")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ’¬ é—®é¢˜: ").strip()
            
            # é€€å‡ºå‘½ä»¤
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            # æ¸…å±å‘½ä»¤
            if user_input.lower() == 'clear':
                print("\033[2J\033[H")  # æ¸…å±
                continue
            
            # ç©ºè¾“å…¥
            if not user_input:
                continue
            
            # æ„å»ºæç¤º
            prompt = f"{instruction}\né—®é¢˜ï¼š{user_input}\nå›ç­”ï¼š"
            
            # ç”Ÿæˆå›ç­”
            inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
            
            print("\nğŸ¤” æ€è€ƒä¸­...")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=512,
                    do_sample=True,
                    top_p=0.8,
                    temperature=0.8,
                    repetition_penalty=1.1
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–å›ç­”éƒ¨åˆ†ï¼ˆå»æ‰æç¤ºï¼‰
            if "å›ç­”ï¼š" in response:
                response = response.split("å›ç­”ï¼š")[-1].strip()
            
            print(f"\nğŸ¥ å›ç­”: {response}")
            print("-" * 50)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            continue


def main():
    parser = argparse.ArgumentParser(description="äº¤äº’å¼æ¨ç†æµ‹è¯•")
    parser.add_argument(
        '--model_path',
        type=str,
        required=True,
        help='æ¨¡å‹è·¯å¾„ï¼Œå¦‚ outputs/lora_medical/checkpoint-best'
    )
    parser.add_argument(
        '--base_model_path',
        type=str,
        default=None,
        help='åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæ˜¯ LoRA æ¨¡å‹éœ€è¦æä¾›ï¼‰'
    )
    parser.add_argument(
        '--instruction',
        type=str,
        default="å›ç­”åŒ»ç–—å¥åº·é—®é¢˜",
        help='æŒ‡ä»¤æç¤º'
    )
    
    args = parser.parse_args()
    
    print("\nğŸš€ åŠ è½½æ¨¡å‹...")
    print(f"   æ¨¡å‹è·¯å¾„: {args.model_path}")
    if args.base_model_path:
        print(f"   åŸºç¡€æ¨¡å‹: {args.base_model_path}")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_trained_model(args.model_path, args.base_model_path)
    
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    
    # å¼€å§‹å¯¹è¯
    chat(model, tokenizer, args.instruction)


if __name__ == "__main__":
    main()
